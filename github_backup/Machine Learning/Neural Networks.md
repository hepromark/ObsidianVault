What is a Neural Network?

A neural network is a [[Graph Theory#Graph Terminology|Directed Acrylic Graph]] made up of Neurons. Depending on the input values, individual neurons may be 'on' or 'off', which allows the network to process and transmit information.

### Components of a Neural Network

1. Neurons / Nodes:
	- Basic unit of computation that: Takes input + applies weights, bias & an activation function -> gives output. 

2. Layers:
	- A group of neuron's that takes same input and outputs to the same source
	
	A simple neural network usually has 3 types of layers:
	- Input layer: Raw data input to network
	- Hidden layer: Intermediate layers, where bulk of computation happens
	- Output layer: Final layer that produces a result.

### Inference

Inference / forward propagation is the process of 'using' the network to process input
- Input data passed through the network layer by layer, and output value(s) generated by the last layer

Usually, the network used for inference is the same as the network for learning, but not always. [[TensorRT]] is a library to optimize / compress the neural network during inference to speed up inference times. 

### Training

Training is the process of modifying the weights & biases in the neural network to minimize a certain [[Cost Function]]. 
- The minimization step is essentially a search problem; the search space is a high-dimensional space representing the cost for every parameter choice. [[Gradient Descent]] is usually the algorithm chosen to search this space for the optimal parameters.
- In [[Supervised Learning]], the neural network changes the parameters by comparing the network's output (prediction) $\hat{y}$ versus the ground truth ${y}$. 
- [[Back Propagation]] is the algorithm that connects error $\hat{y}$ vs $y$ to how much to update each parameter

### Network Architectures

The architecture of the network essentially determines how its components (layers, neutrons, etc..) are arranged. It is a very important design choice in how the network works, and defines the flow of information through the network. 
- Different architectures are appropriate for differently 'shaped' data
- Research is always ongoing in novel new network architectures
- Efficient architecture choices lead better models with less parameters

1. [[Feedforward Neural Networks|Feedforward Neural Networks]] 
	- Simplest type of neural network, where information passed in one direction from input layer to output layer. 
	- Ex: [[Multi-layer Perceptron]]

2. [[Convolutional Neural Networks]]
	- For spatial data (ex: images)
	- Also feedforward, but with special convolutional layers where a 'kernel' is used to 'scan' the incoming activation matrix.
	- Ex: ResNet 

3. [[Recurent Neural Networks]]
	- For sequential / temporal data (ex: text, sounds)
	- Uses layers with feedback loops, so that network can retain information from earlier steps
	- Ex: LSTMs, GRUs

4. [[Transformer Networks]]
	- Modern, specialize networks for sequential data where information from any part of the sequence can affect another much further away part of the sequence
	- Uses self-attention mechanisms (instead of LSTM type attention) to learn dependencies between parts of input sequence.
	- Ex: GPT, BERT

5. [[Generative Adversial Networks]] 
	- Made from 2 networks- generator and discriminator, that competes with each other to generate realistic data.
